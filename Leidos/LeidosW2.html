<html>
    <head>
        <title>Leidos Inernship Homepage</title>
        <link href='https://fonts.googleapis.com/css?family=Raleway' rel='stylesheet'>
        <link rel="stylesheet" type="text/css" href="leidos.css">
    </head>
    <body>
      <div class="navbar">
        <a href="..\index.html">Home</a>
        <a href="index.html">Leidos Dynetics</a>
        <div class="dropdown">
          <button class="dropbtn">Weeks
            <i class="fa fa-caret-down"></i>
          </button>
          <div class="dropdown-content">
            <a href="LeidosW1.html">Week 1 - 06/17</a>
            <a href="LeidosW2.html">Week 2 - 06/24</a>
            <a href="LeidosW1.html">Week 3 - 07/01</a>
          </div>
        </div>
        <a href="AtLB.html">Book Club</a>
        <a href="Journal.html">Journal Club</a>
      </div>


        <h1>Week 1: 06/24 - 06/28</h1>

        <div class = "content">
          <div class = "intros">
            <p>This week can be defined by three words: errors, debugging, and llamas. During my second week at Leidos Dynetics, I settled more into my day to day workflow at the company and thoroughly enjoyed everything from routine tasks to struggles I would have at the job. As stated at the end of last week, my project changed from working with image transformers such as BLIP 2 to text-to-text transformers, in this case, Meta’s LLaMA. Despite the seemingly simple instructions offered on <a href = "https://github.com/meta-llama/llama">Meta’s GitHub Page</a>, downloading a large language model proved to be a very difficult but rewarding process. Through the many errors, forms, and endless debugging, this experience ended up being one of my favorites in my (limited) coding career and I’m excited to continue my work coming into these next few weeks.</p>
          </div>
          <hr>
          <h2>Day by Day</h2>
          <p>What I neglected to mention on Friday of last week’s post was that I had spent a majority of my afternoon researching large language models (LLM) and, though I had forgotten to save my excel file explaining my choices, I ultimately chose LLaMA 2 as my model of choice. The other two front-running choices, Stanford’s Alpaca and Bienvenue ’s Minstral, are both built off the LLaMA model and since I was already going to fine-tune our model, I thought it would be the best to go with the original and make any edits to it myself.  </p>
          <p>On Monday, after discussing the idea with my Mundy who agreed with my thought process, I looked into which version of LLaMA to download. A quick Google search informed me that the decision between LLaMA 2 and LLaMA 3 was a mere illusion of choice. My search question was based on the question of how much GPU power does it take to run a billion parameters. Though the question is quite simplified and would vary highly based on the model and architecture, the rough estimate was about 4 GB for every one billion parameters. Though the work laptop I was provided technically could’ve run LLaMA 3’s lightest weight model at 8 billion parameters, a quick check to see how much power I had available (31.2 GB) proved that LLaMA 2 would be the better option. </p>
          <p>The morning mainly consisted of me following the instructions found on the GitHub page until I had to run the shell file. I was informed by a fellow intern that those files need to be run in a Linux terminal and I spent my morning looking for a bypass. Mundy recommended using the Git Bash terminal which, though it was able to read and begin the process of the shell file, couldn’t wget (a terminal command) the link I was providing to download my model. The afternoon mainly consisted of me trying to bypass this issue. After reading dozens of threads, my struggles were finally fixed by the revelation that the wget command had to be native to the terminal itself and couldn’t just exist in an environment in the terminal. Though I was able to get everything set up to solve the problem, the actual moving of the wget.exe file into the Git Bash terminal required me to have Development Enabled access on my laptop, something that I was already struggling with and an issue that had to be addressed tomorrow as I had run out of time.          </p>
          <p>Tuesday morning started swiftly with the revelation that I did indeed have Dev Enabled access on my laptop, I just didn’t know how to use it (thank you IT). Each version of LLaMA took about 45 minutes to download and the environment needed to run the example programs was also not the easiest thing to get set up. That being said, trying to get the example program to run was an issue that did take the rest of the day with little to no improvements throughout the entire process.</p>
          <p>Wednesday started with addressing the first issue that arises when trying to run the example program: SSL verification. Mundy and I started the day with an hour and a half of diving through forums trying to figure out how to overcome this error. Though I wish the solution was found buried somewhere in those posts, what ended up working was me (on a whim) connecting back to the VPN meant to be causing the issue. The second error that morning was trying to download an older version of numpy so all dependencies could work properly. I ended up spending practically all of my afternoon trying to get out of “dependency hell” to no avail. I ended the day by resolving to download everything manually to see exactly which programs were causing the error, promptly starting with the first program I had tried to download: numpy. The fact that python refused to download a version of numpy under 2.0.0 even on a clean environment led me to believe that the issue wasn’t with numpy but the python version I was using itself, and a few tests later quickly confirmed this. The last notes for myself at the end of Wednesday were that I would have to abandon python 3.11.9 and 3.8.19 (the two versions I had already downloaded on my laptop) and move to either 3.9.X or 3.10.X. </p>
          <p>Throughout the entire time, debugging this seemingly simple issue was admittedly very frustrating, though by the end of the day and in hindsight, it was also very rewarding. It forced me to track my debugging process in a more organized manner so that I could follow my thoughts days later as I write this. It forced me to be thorough with the way I read through error messages and forums meant to answer those errors because a lot of the information provided, whether in the debugger or online, ended up being extraneous details from the matter at hand. It finally proved to be very rewarding to hone my ability to problem solve. Restarting could've been a disheartening state to be in after an entire day of debugging but it quickly proved to be the most significant thing I did that day as it quickly proved the issue wasn’t with my process but the version of python itself. Though I hope to not go through dependency hell anytime in the near future, the process paid off in the end that I’m happy to have worked through.</p>
          <p>I came into work early Thursday morning determined to finish off any remaining errors and to get a model running by the end of the day. After creating the new environment in Python 3.10.11 and downloading the dependencies again, the only remaining errors came with torch trying to use NCCL. A Google search showed this was needed to use GPU storage and I tried to follow the proper process of getting NCCL set up on my laptop.  It turned out that changing “nccl” to “gloo” in the generation script of the process fixed the issue. What is gloo? Where did gloo come from? Where did gloo go? [Where did gloo come from Cotton Eye Joe?] All questions that I will remain blissfully ignorant to as long as the program continues to run. </p>
          <p>That being said, my days at Leidos Dynetics are far from constant work. In between talking with my fellow interns and lunch breaks, the company also holds a multitude of events for us to attend. Thursday was especially full of things to do, with both a Lunch-and-Learn and a company-wide paper rocket making competition. The Lunch-and-Learn staff taught our intern group about the Aviation & Drones team, where they not only presented to us about what they did and showed their workshop (by far the most impressive workshop I’ve been in), but they took us out to their testing fields to show some of their drones in action. The third annual rocket making competition was also fun even though I opted out of participating this year. I did make sure to stop by for snowcones though and managed to win a free hat as part of a raffle on my way out!          </p>
          <p>Friday started out with results! The program ran without direct errors producing a terminal full of mostly nonsense right before we started Journal and Book Club. After editing the python file to include a timer, the example chat program printed out more errors in thirteen minutes, a simultaneously exciting and worrying fact. In the afternoon, I continued my test and was able to run the text completion example in only two and a half minutes with no errors. During the first attempt of example chat completion right after lunch, I was also able to get it to run without errors but I have no proof as I decided to not save this run to a text file when running it. I spent the rest of the afternoon trying to recreate this miraculous response to no avail until the end of the day when another intern asked me to check my GPU usage. After only a few seconds of running, the example chat program had not only maxed out my GPU, but my CPU as well: a likely culprit for the nonsense errors it was providing and a likely answer as to why it was able to run immediately after lunch when my computer had been off for a few minutes. Though the breakthrough was exciting and made me want to continue testing right then and there, it was already past the end of the day and with each test taking fifteen minutes, it didn’t make the most sense to stay for the weekend to figure out this issue, leaving me with an easy course of action for Monday. </p>
          <p>Week two ended up being my most productive and fun week at Leidos (taken from a very large sample size as you can see). The days were always productive even if most of my searches led to dead ends and the time I got to spend outside of this main project were some of the most memorable parts of the week. Though I’m sure you’re tired of my rambling about this week, I want to leave this entry with one last thing: Daniel’s Process on how to Run LLaMA 2. Though I can almost guarantee these steps won’t work for your machine verbatim, I hope you can use my process to hopefully set up your own experiments someday in the future. </p>
          <!-- <div class = "intros">
            <p>Prerequisites: Git Bash Terminal
              Go to the LLaMA 2 GitHub page and follow it to the Meta Website to get access to the LLaMA 2 (and if you want, LLaMA 3) models
              Git clone the LLaMA 2 repository onto your own device
              This process and all procedeeing processes are mirrored for LLaMA 3
              Enter the LLaMA 2 repository (it will be downloaded as just “llama”) and run the download.sh file
              If you are not already in a Linux Terminal, you can use the Git Bash terminal to run this file using the command “. download.sh”
              The Git Bash does not have wget inherit to it so in order to get it installed, downloaded it as an .exe file and move it into the “bin” folder in you laptop
              The easiest way to do this is to use the “Properties” option when right clicking your Git Bash program, coping its path until you get the the “Git” folder, then pasting that into your top navigation bar in your file explorer; from there enter the “mingw64” folder, then the “bin” folder, and move your “wget.exe” executable inside 
              Wget executable can be found here [not verified source]
              When the shell file prompts you for a URL, copy and paste the one you received from your email and specify the models you want to download
              They are limited in both the times you can use them and how long they last after request so don’t be afraid to request them again
              In the same folder, it would also be best to set up an virtual environment to run LLaMA; to do this, in the same folder holding the repository (for convenience) run the command “python -m venv path/to/this/folder” to create a virtual environment
              Ideal python versions for this virtual environment are Python 3.10.X or 3.9.X for the simple reason that they are the most upgraded version of Python that will allow you to download a lower version of numpy
              Activate the environment by entering the “Scripts” folder and running “. activate” in Linux or “activate.bat” in the command prompt
              With the environment active, enter the “llama” folder and run the command “pip install -e .”
              After this is done, use “pip list” to check that pip installed a lower version of numpy; if it did not, use “pip uninstall numpy” and “pip install numpy==1.24.4” to install the right version
              By this step, the environment to run LLaMA is set up. That being said, the example program does take a bit more finagling. 
              </p>
          </div> -->
        </div>
    </body>
</html>