Hey!  
  
I thought I’d host a small Journal Club every Thursday afternoon at 2 pm (in the intern room), where we’ll discuss papers/articles that are foundational in the field of artificial intelligence and computer science.  Our tentative reading list for the next couple of weeks are as follows:  
  
Week 1 (6/27) – Temperature as Joules per Bit ([https://arxiv.org/pdf/2401.12119](https://arxiv.org/pdf/2401.12119 "https://arxiv.org/pdf/2401.12119"))  
  
Week 2 (7/4) - Toy Models of Superposition ([https://transformer-circuits.pub/2022/toy_model/index.html](https://transformer-circuits.pub/2022/toy_model/index.html "https://transformer-circuits.pub/2022/toy_model/index.html"))  
  
Week 3 (7/11) - Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective Attention, Curiosity & Creativity ([https://arxiv.org/pdf/0709.0674](https://arxiv.org/pdf/0709.0674 "https://arxiv.org/pdf/0709.0674"))  
  
Week 4 (7/18) – Zoom In: An Introduction to Circuits ([https://distill.pub/2020/circuits/zoom-in/](https://distill.pub/2020/circuits/zoom-in/))  
  
Week 5 (7/25) – Feature Visualization ([https://distill.pub/2017/feature-visualization/](https://distill.pub/2017/feature-visualization/))  
  
--- Daniel Leaves ---  
--- Focus on Mechanistic Interpretability Training ---  
  
Week (8/1) – Skip  
  
Week (8/8) – The Building Blocks of Interpretability ([https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/))  
  
Week (8/15) – Exploring Neural Networks with Activation Atlases ([https://distill.pub/2019/activation-atlas/](https://distill.pub/2019/activation-atlas/))  
  
Week (8/22) – Reflections on Qualitative Research ([https://transformer-circuits.pub/2024/qualitative-essay/index.html](https://transformer-circuits.pub/2024/qualitative-essay/index.html))  
  
Week (8/29) – Weight Banding ([https://distill.pub/2020/circuits/weight-banding/](https://distill.pub/2020/circuits/weight-banding/))  
  
Week () – Branch Specialization ([https://distill.pub/2020/circuits/branch-specialization/](https://distill.pub/2020/circuits/branch-specialization/))  
  
Week () – Visualizing Weights ([https://distill.pub/2020/circuits/visualizing-weights/](https://distill.pub/2020/circuits/visualizing-weights/))  
  
Week () – Curve Circuits ([https://distill.pub/2020/circuits/curve-circuits/](https://distill.pub/2020/circuits/curve-circuits/))  
  
Week () – High-Low Frequency Detectors ([https://distill.pub/2020/circuits/frequency-edges/](https://distill.pub/2020/circuits/frequency-edges/))  
  
Week () – Naturally Occurring Equivariance in Neural Networks ([https://distill.pub/2020/circuits/equivariance/](https://distill.pub/2020/circuits/equivariance/))  
  
Week () – Curve Detectors ([https://distill.pub/2020/circuits/curve-detectors/](https://distill.pub/2020/circuits/curve-detectors/))  
  
Week () – An Overview of Early Vision in InceptionV1 ([https://distill.pub/2020/circuits/early-vision/](https://distill.pub/2020/circuits/early-vision/))  
  
Week () – Multimodal Neurons in Artificial Neural Networks ([https://distill.pub/2021/multimodal-neurons/](https://distill.pub/2021/multimodal-neurons/))  
  
Week () – A Mathematical Framework for Transformer Circuits ([https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html))  
  
Week () – In-context Learning and Induction Heads ([https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html))  
  
Week () – Softmax Linear Units ([https://transformer-circuits.pub/2022/solu/index.html](https://transformer-circuits.pub/2022/solu/index.html))  
  
Week () – Reread - Toy Models of Superposition ([https://transformer-circuits.pub/2022/toy_model/index.html](https://transformer-circuits.pub/2022/toy_model/index.html "https://transformer-circuits.pub/2022/toy_model/index.html"))  
  
Week () – Superposition, Memorization, and Double Descent ([https://transformer-circuits.pub/2023/toy-double-descent/index.html](https://transformer-circuits.pub/2023/toy-double-descent/index.html))  
  
Week () – Privileged Bases in the Transformer Residual Stream ([https://transformer-circuits.pub/2023/privileged-basis/index.html](https://transformer-circuits.pub/2023/privileged-basis/index.html))  
  
Week () – Towards Monosemanticity: Decomposing Language Models With Dictionary Learning ([https://transformer-circuits.pub/2023/monosemantic-features/index.html](https://transformer-circuits.pub/2023/monosemantic-features/index.html))  
  
Week () – Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet ([https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html))  
  
Tabled Articles:  
  
What is important about the No Free Lunch theorems? ([https://arxiv.org/pdf/2007.10928](https://arxiv.org/pdf/2007.10928 "https://arxiv.org/pdf/2007.10928"))  
  
Why Should Philosophers Care About Computational Complexity ([https://www.scottaaronson.com/papers/philos.pdf](https://www.scottaaronson.com/papers/philos.pdf "https://www.scottaaronson.com/papers/philos.pdf"))  
  
Your participation is optional.  If do you choose to participate, please come prepared having read the paper (or most of it) with some notes, questions, and points of discussion :)  
  
-Mundy